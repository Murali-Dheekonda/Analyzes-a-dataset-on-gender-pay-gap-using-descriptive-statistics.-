{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"16AD1DNADQHVjQrA-ryB6xdXspFmWC-Bv","authorship_tag":"ABX9TyNEiwqMjfEGH4jo7SHygfAv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vs9TDKP-EcsT","executionInfo":{"status":"ok","timestamp":1744899027716,"user_tz":-60,"elapsed":1538,"user":{"displayName":"Murali Dheekonda","userId":"09835857048925522524"}},"outputId":"32d48748-6b51-4d48-f37d-9833818999cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Base Pay: 94472.653\n","Median Age: 41.0\n","Mode Job Title: Marketing Associate\n","\n","Mean Base Pay by Gender:\n"," gender\n","Female    89942.818376\n","Male      98457.545113\n","Name: basePay, dtype: float64\n"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","\n","# Load dataset (already loaded previously, shown here for completeness)\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Gender pay gap.csv')\n","\n","# --- MEAN ---\n","# Calculate the mean base salary\n","mean_basePay = df['basePay'].mean()\n","print(\"Mean Base Pay:\", mean_basePay)\n","\n","# --- MEDIAN ---\n","# Calculate the median age\n","median_age = df['age'].median()\n","print(\"Median Age:\", median_age)\n","\n","# --- MODE ---\n","# Calculate the mode of job titles\n","mode_jobTitle = df['jobTitle'].mode()[0]\n","print(\"Mode Job Title:\", mode_jobTitle)\n","\n","# --- OPTIONAL: Grouping Example (Extra) ---\n","# Mean basePay by gender\n","grouped_mean_salary = df.groupby('gender')['basePay'].mean()\n","print(\"\\nMean Base Pay by Gender:\\n\", grouped_mean_salary)"]},{"cell_type":"code","source":["The code analyzes a dataset on gender pay gap using descriptive statistics.  The selection of `basePay`, `age`, and `jobTitle` attributes is driven by common exploratory data analysis practices and the likely relevance to understanding pay disparity.\n","\n","* **`basePay`**:  This is the central attribute for investigating the gender pay gap. Calculating its *mean* provides a general overview of average salary, crucial for initial comparisons across demographics.  Further analysis (shown in the optional grouping example) uses `basePay` alongside `gender` to directly compare mean salaries between genders, revealing potential pay discrepancies.\n","\n","* **`age`**:  Age often correlates with salary and experience.  The *median* age is used instead of the mean as it's less sensitive to outliers, providing a more robust representation of the typical employee age within the dataset.  Understanding the age distribution is essential, as differences in average age between gender groups could influence pay gap observations.\n","\n","* **`jobTitle`**:  Job title strongly influences salary.  The *mode* provides the most frequent job title in the dataset, highlighting the dominant role within the organization.  While the mode itself may not directly reveal pay disparities, further investigation using job title as a grouping factor could reveal significant variations in pay across different roles and potential gender representation within those roles.\n","\n","The optional grouping by `gender` further enhances the initial analysis by directly comparing the mean base pay between different genders. This gives more focused insights into salary differences based on gender, building upon the general statistics calculated earlier. The selection of descriptive statistics (mean, median, mode) is appropriate for summarizing the respective attributes and identifying potential trends for further investigation.\n"],"metadata":{"id":"7HTbS1tGWcGR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","\n","# Load dataset (HR Dataset)\n","hr_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/HR_Dataset.csv')\n","\n","# --- MEAN ---\n","# Calculate the mean salary\n","mean_salary = hr_df['Salary'].mean()\n","print(\"Mean Salary:\", mean_salary)\n","\n","# --- MEDIAN ---\n","# Calculate the median number of absences\n","median_absences = hr_df['Absences'].median()\n","print(\"Median Absences:\", median_absences)\n","\n","# --- MODE ---\n","# Calculate the mode of employee positions\n","mode_position = hr_df['Position'].mode()[0]\n","print(\"Mode Position:\", mode_position)\n","\n","# --- OPTIONAL: Grouping Example (Extra) ---\n","# Mean salary by Department\n","grouped_mean_salary_by_dept = hr_df.groupby('Department')['Salary'].mean()\n","print(\"\\nMean Salary by Department:\\n\", grouped_mean_salary_by_dept)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koXkCm_zGlck","executionInfo":{"status":"ok","timestamp":1744899315182,"user_tz":-60,"elapsed":982,"user":{"displayName":"Murali Dheekonda","userId":"09835857048925522524"}},"outputId":"32405a42-eb9e-4e4b-fc32-32ce0acfefa0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Salary: 69020.6848874598\n","Median Absences: 10.0\n","Mode Position: Production Technician I\n","\n","Mean Salary by Department:\n"," Department\n","Admin Offices            71791.888889\n","Executive Office        250000.000000\n","IT/IS                    97064.640000\n","Production               59953.545455\n","Sales                    69061.258065\n","Software Engineering     94989.454545\n","Name: Salary, dtype: float64\n"]}]},{"cell_type":"code","source":["# prompt: In the above code, discuss about the selected attribute(s) for the purpose and rational behind it\n","# within 200- 250 words\n","\n","The provided code analyzes two datasets: a gender pay gap dataset and an HR dataset, using descriptive statistics.  The choice of attributes and descriptive statistics is again driven by the goal of exploring potential trends and disparities within the data.  Let's examine the attributes and statistics used in each:\n","\n","**Gender Pay Gap Dataset:**\n","\n","*   `basePay`:  This remains the key attribute, and its mean is crucial for establishing a baseline average salary. The subsequent grouping by `gender` directly addresses the central research question of pay disparity.\n","\n","*   `age`:  Median age provides a robust representation of the typical employee age, considering potential outliers in age and salary. The rationale is that age and experience usually correlate with salary, so the age distribution's effect on the pay gap needs to be considered.\n","\n","*   `jobTitle`: The mode of `jobTitle` identifies the most common job role, providing context for the overall salary analysis. Further investigation would involve analyzing salary distributions by `jobTitle`, considering gender representation within roles to assess potential disparities.\n","\n","**HR Dataset:**\n","\n","*   `Salary`:  The mean salary is a standard measure used to establish a general overview of compensation levels within the organization.\n","\n","*   `Absences`:  The median number of absences provides a robust measure of employee absenteeism, less susceptible to outliers compared to the mean. This is relevant for HR analytics, potentially indicating employee well-being or performance issues.\n","\n","*   `Position`: The mode of `Position` indicates the most prevalent job role in the organization, serving as a point of reference for understanding the overall workforce composition.  Further analyses could investigate how salary or absenteeism varies across different positions.\n","\n","\n","In both cases, the selection of descriptive statistics (mean, median, mode) caters to identifying central tendencies and distributions for respective attributes. The additional grouping by `gender` (in the gender pay gap dataset) and by `Department` (in the HR dataset) allows for more granular analysis to understand how the key variables vary across groups, uncovering potential patterns and disparities.\n"],"metadata":{"id":"Kox9OOUSXPTv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: for the above data set  /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Asian_American_Quality_of_Life.csv\n","#  Selecting attribute(s) to perform the following “Measure of central tendency”:\n","# • Mean, Median and Mode.\n","\n","import pandas as pd\n","\n","# Load the Asian American Quality of Life dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Asian_American_Quality_of_Life.csv')\n","\n","# Select relevant numerical columns for central tendency measures\n","numerical_cols = df.select_dtypes(include=['number']).columns\n","\n","# --- MEAN ---\n","# Calculate the mean for each numerical column\n","mean_values = df[numerical_cols].mean()\n","print(\"Mean:\\n\", mean_values)\n","\n","# --- MEDIAN ---\n","# Calculate the median for each numerical column\n","median_values = df[numerical_cols].median()\n","print(\"\\nMedian:\\n\", median_values)\n","\n","# --- MODE ---\n","# Calculate the mode for each numerical column\n","mode_values = df[numerical_cols].mode().iloc[0] # Take the first mode if multiple exist\n","print(\"\\nMode:\\n\", mode_values)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5cS9yCcHUfO","executionInfo":{"status":"ok","timestamp":1744902127442,"user_tz":-60,"elapsed":222,"user":{"displayName":"Murali Dheekonda","userId":"09835857048925522524"}},"outputId":"d3eb077f-ec64-4588-9996-06cd536647f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean:\n"," Survey ID                  2.105820e+06\n","Age                        4.285192e+01\n","Education Completed        1.507815e+01\n","Household Size             3.295139e+00\n","Grandparent                0.000000e+00\n","Other Relative             0.000000e+00\n","Other                      0.000000e+00\n","Self Employed Full Time    0.000000e+00\n","Self Employed Part Time    0.000000e+00\n","Disabled                   0.000000e+00\n","Unemployed                 0.000000e+00\n","Other Employement          0.000000e+00\n","Achieving Ends Meet        1.722449e-01\n","Duration of Residency      1.563509e+01\n","Primary Language           6.675697e-01\n","Discrimination             3.027923e-01\n","Hygiene Assistance         2.705837e-02\n","Smoking                    6.138996e-02\n","Drinking                   3.320463e-02\n","Regular Exercise           6.178737e-01\n","Healthy Diet               8.086420e-01\n","Heart Disease              0.000000e+00\n","Stroke                     0.000000e+00\n","Cancer                     0.000000e+00\n","Hepatitis                  0.000000e+00\n","Kidney Problem             0.000000e+00\n","Asthma                     0.000000e+00\n","COPD                       0.000000e+00\n","users                      8.040435e-01\n","Other                      4.524362e-02\n","Quality of Life            7.669006e+00\n","Psychiatrist               0.000000e+00\n","Therapist/Counselor        0.000000e+00\n","See Family                 3.085115e+00\n","Close Family               2.466277e+00\n","Helpful Family             2.808652e+00\n","See Friends                3.194488e+00\n","Close Friends              2.454510e+00\n","Helpful Friends            2.689856e+00\n","Residency                  9.273484e+00\n","Other Transportation       1.010494e-02\n","dtype: float64\n","\n","Median:\n"," Survey ID                  50160.0\n","Age                           40.0\n","Education Completed           16.0\n","Household Size                 3.0\n","Grandparent                    0.0\n","Other Relative                 0.0\n","Other                          0.0\n","Self Employed Full Time        0.0\n","Self Employed Part Time        0.0\n","Disabled                       0.0\n","Unemployed                     0.0\n","Other Employement              0.0\n","Achieving Ends Meet            0.0\n","Duration of Residency         13.0\n","Primary Language               1.0\n","Discrimination                 0.0\n","Hygiene Assistance             0.0\n","Smoking                        0.0\n","Drinking                       0.0\n","Regular Exercise               1.0\n","Healthy Diet                   1.0\n","Heart Disease                  0.0\n","Stroke                         0.0\n","Cancer                         0.0\n","Hepatitis                      0.0\n","Kidney Problem                 0.0\n","Asthma                         0.0\n","COPD                           0.0\n","users                          1.0\n","Other                          0.0\n","Quality of Life                8.0\n","Psychiatrist                   0.0\n","Therapist/Counselor            0.0\n","See Family                     3.0\n","Close Family                   3.0\n","Helpful Family                 3.0\n","See Friends                    3.0\n","Close Friends                  3.0\n","Helpful Friends                3.0\n","Residency                      5.0\n","Other Transportation           0.0\n","dtype: float64\n","\n","Mode:\n"," Survey ID                  10919.0\n","Age                           25.0\n","Education Completed           17.0\n","Household Size                 2.0\n","Grandparent                    0.0\n","Other Relative                 0.0\n","Other                          0.0\n","Self Employed Full Time        0.0\n","Self Employed Part Time        0.0\n","Disabled                       0.0\n","Unemployed                     0.0\n","Other Employement              0.0\n","Achieving Ends Meet            0.0\n","Duration of Residency          1.0\n","Primary Language               1.0\n","Discrimination                 0.0\n","Hygiene Assistance             0.0\n","Smoking                        0.0\n","Drinking                       0.0\n","Regular Exercise               1.0\n","Healthy Diet                   1.0\n","Heart Disease                  0.0\n","Stroke                         0.0\n","Cancer                         0.0\n","Hepatitis                      0.0\n","Kidney Problem                 0.0\n","Asthma                         0.0\n","COPD                           0.0\n","users                          1.0\n","Other                          0.0\n","Quality of Life                8.0\n","Psychiatrist                   0.0\n","Therapist/Counselor            0.0\n","See Family                     3.0\n","Close Family                   3.0\n","Helpful Family                 3.0\n","See Friends                    3.0\n","Close Friends                  3.0\n","Helpful Friends                3.0\n","Residency                      1.0\n","Other Transportation           0.0\n","Name: 0, dtype: float64\n"]}]},{"cell_type":"code","source":["# prompt: In the above code, discuss about the selected attribute(s) for the purpose and rational behind it\n","# within 200- 250 words\n","\n","# The code analyzes the Asian American Quality of Life dataset using descriptive statistics.\n","# The selection of all numerical columns is a broad approach aiming to provide an overview of central tendencies across various aspects of quality of life.\n","\n","# --- MEAN ---\n","# Calculating the mean for each numerical column provides an average value for each factor. This helps understand the typical level of each quality of life indicator.\n","# For example, the mean income provides an overview of the average income in the dataset.\n","\n","# --- MEDIAN ---\n","# The median is used because it's a robust measure less affected by outliers.  Certain quality of life indicators might have skewed distributions (e.g., income or wealth).\n","# The median offers a better representation of the 'typical' value in the presence of extreme values, providing a more balanced view compared to the mean.\n","\n","# --- MODE ---\n","# The mode reveals the most frequent value for each numerical column. This can be particularly insightful for categorical variables that have been numerically coded.\n","# In the context of a quality of life dataset, the mode might indicate the most common level of education or a prevalent housing situation.\n","# For continuous numerical variables, the mode may pinpoint concentration points within the distribution.  However, for truly continuous data (e.g. income or age), the mode may not be meaningful and it may be more useful to consider other measures of central tendency.  The code handles cases with multiple modes by taking the first occurring mode.\n","\n","# Overall, the comprehensive analysis using mean, median, and mode for all numerical columns facilitates a thorough initial exploration of the dataset.\n","# This exploration provides a foundational understanding of the distribution and central tendencies of various quality of life aspects within the Asian American community.  Further investigation will involve other statistical approaches and potentially visualization techniques.\n"],"metadata":{"id":"qaGH4GbvXaD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Provisional_COVID-19_Deaths.csv\n","# Selecting attribute(s) to perform the following “Measure of central tendency”:\n","# • Mean, Median and Mode.\n","\n","import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Provisional_COVID-19_Deaths.csv')\n","\n","# Select the columns you want to analyze (replace with your actual column names)\n","columns_to_analyze = ['COVID-19 Deaths', 'Total Deaths'] # Example columns, replace with your choices\n","\n","# Calculate Mean, Median, and Mode for each selected column\n","for column in columns_to_analyze:\n","    if pd.api.types.is_numeric_dtype(df[column]):\n","        mean_val = df[column].mean()\n","        median_val = df[column].median()\n","        mode_val = df[column].mode().iloc[0] if not df[column].mode().empty else \"No unique mode\"  # Handle cases with no unique mode\n","\n","        print(f\"Results for column: {column}\")\n","        print(f\"Mean: {mean_val}\")\n","        print(f\"Median: {median_val}\")\n","        print(f\"Mode: {mode_val}\")\n","        print(\"-\" * 20)\n","    else:\n","        print(f\"Column '{column}' is not numeric, cannot calculate mean, median, and mode.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ebYJR6CRj3E","executionInfo":{"status":"ok","timestamp":1744902236277,"user_tz":-60,"elapsed":1593,"user":{"displayName":"Murali Dheekonda","userId":"09835857048925522524"}},"outputId":"c6b01a0b-69d3-44cf-855f-a70e128b7ee5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Results for column: COVID-19 Deaths\n","Mean: 367.0516483376448\n","Median: 10.0\n","Mode: 0.0\n","--------------------\n","Results for column: Total Deaths\n","Mean: 2830.7697103562336\n","Median: 150.0\n","Mode: 0.0\n","--------------------\n"]}]},{"cell_type":"code","source":["# prompt: In the above code, discuss about the selected attribute(s) for the purpose and rational behind it\n","# within 200- 250 words\n","\n","The code analyzes multiple datasets using descriptive statistics (mean, median, mode). The choice of attributes and statistics depends on the dataset and the goals of the analysis.\n","\n","**General Rationale:**\n","\n","*   **Mean:** Provides the average value, useful for understanding the typical value of a numerical attribute. However, it's sensitive to outliers.\n","\n","*   **Median:** Represents the middle value when data is ordered, less sensitive to outliers than the mean. It provides a more robust measure of central tendency, especially when dealing with skewed distributions.\n","\n","*   **Mode:** Indicates the most frequent value. It's particularly useful for categorical data or identifying peaks in the distribution of numerical data.  For continuous data, the mode might not be as meaningful.\n","\n","**Dataset-Specific Rationale:**\n","\n","1.  **Gender Pay Gap Dataset:**  `basePay`, `age`, and `jobTitle` are selected to explore potential gender-based salary differences. `basePay` (mean) is the central measure. `age` (median) accounts for potential outliers. `jobTitle` (mode) provides context regarding the most prevalent job role. Grouping by gender allows direct comparison.\n","\n","2.  **HR Dataset:** `Salary` (mean), `Absences` (median), and `Position` (mode) are chosen to understand employee compensation, absenteeism, and job role distribution. Median `Absences` mitigates outlier effects.  Grouping by department provides additional insight.\n","\n","3.  **Asian American Quality of Life Dataset:** All numerical columns are analyzed to give an overview of several quality of life aspects.  Both mean and median are used because some indicators may have skewed distributions. The mode provides information about frequent values, especially for potentially categorical or discrete variables.\n","\n","4.  **Provisional COVID-19 Deaths Dataset:**  The code calculates mean, median, and mode for specified columns (`COVID-19 Deaths`, `Total Deaths`).  The choice of columns depends on the specific research questions. The inclusion of a check for numeric data type ensures the code handles non-numeric columns gracefully, preventing errors.\n","\n","\n","In summary, the choice of descriptive statistics is determined by the nature of the data and the research questions.  The code uses mean to represent typical values, median for robustness against outliers, and mode to identify frequent values, giving a comprehensive initial exploration of the datasets. The additional grouping in some cases allows deeper analysis of the relationships between variables.\n"],"metadata":{"id":"Bq7HC3fHXixV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Selecting attribute(s) to perform the following “Measure of Spread/Dispersion”: Range,\n","# Variance and Standard Deviation.\n","# perform with the above all data sets with their paths\n","\n","import pandas as pd\n","\n","# Load datasets (replace with your actual file paths if different)\n","file_paths = [\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Gender pay gap.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/HR_Dataset.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Asian_American_Quality_of_Life.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Provisional_COVID-19_Deaths.csv'\n","]\n","\n","for file_path in file_paths:\n","    try:\n","        df = pd.read_csv(file_path)\n","        print(f\"Analyzing: {file_path}\")\n","\n","        # Select numerical columns (excluding object types which are likely categorical)\n","        numerical_cols = df.select_dtypes(include=['number']).columns\n","\n","        for col in numerical_cols:\n","            if len(df[col].dropna()) > 0 : # Check if there are any numerical values to avoid errors.\n","              # Calculate Range, Variance, and Standard Deviation\n","              data_range = df[col].max() - df[col].min()\n","              variance = df[col].var()\n","              std_dev = df[col].std()\n","\n","              print(f\"Column: {col}\")\n","              print(f\"  Range: {data_range}\")\n","              print(f\"  Variance: {variance}\")\n","              print(f\"  Standard Deviation: {std_dev}\")\n","              print(\"-\" * 20)\n","        print(\"\\n\") # Add extra line break for better readability between files\n","    except FileNotFoundError:\n","        print(f\"Error: File not found at {file_path}\")\n","    except Exception as e:\n","        print(f\"An error occurred while processing {file_path}: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPWzYEiqSGPX","executionInfo":{"status":"ok","timestamp":1744902367313,"user_tz":-60,"elapsed":690,"user":{"displayName":"Murali Dheekonda","userId":"09835857048925522524"}},"outputId":"dce3c75e-4673-4764-cc81-0f5dc46d1488"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Gender pay gap.csv\n","Column: age\n","  Range: 47\n","  Variance: 204.3428938938925\n","  Standard Deviation: 14.294855504477564\n","--------------------\n","Column: perfEval\n","  Range: 4\n","  Variance: 2.027658658658644\n","  Standard Deviation: 1.423958798090255\n","--------------------\n","Column: seniority\n","  Range: 4\n","  Variance: 1.9461051051051323\n","  Standard Deviation: 1.3950287112117559\n","--------------------\n","Column: basePay\n","  Range: 145518\n","  Variance: 641988565.313904\n","  Standard Deviation: 25337.493272103773\n","--------------------\n","Column: bonus\n","  Range: 9590\n","  Variance: 4017528.6196986963\n","  Standard Deviation: 2004.377364594476\n","--------------------\n","\n","\n","Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/HR_Dataset.csv\n","Column: EmpID\n","  Range: 310\n","  Variance: 8086.0\n","  Standard Deviation: 89.92218858546538\n","--------------------\n","Column: MarriedID\n","  Range: 1\n","  Variance: 0.24051446945337662\n","  Standard Deviation: 0.4904227456525407\n","--------------------\n","Column: MaritalStatusID\n","  Range: 4\n","  Variance: 0.8897002385644645\n","  Standard Deviation: 0.9432392265827713\n","--------------------\n","Column: GenderID\n","  Range: 1\n","  Variance: 0.2464474639560217\n","  Standard Deviation: 0.4964347529696342\n","--------------------\n","Column: EmpStatusID\n","  Range: 4\n","  Variance: 3.219811222902199\n","  Standard Deviation: 1.794383243039847\n","--------------------\n","Column: DeptID\n","  Range: 5\n","  Variance: 1.173944611554814\n","  Standard Deviation: 1.0834872456816527\n","--------------------\n","Column: PerfScoreID\n","  Range: 3\n","  Variance: 0.3446530442900126\n","  Standard Deviation: 0.5870715836165233\n","--------------------\n","Column: FromDiversityJobFairID\n","  Range: 1\n","  Variance: 0.08482522559900453\n","  Standard Deviation: 0.2912477048819519\n","--------------------\n","Column: Salary\n","  Range: 204954\n","  Variance: 632856381.6100614\n","  Standard Deviation: 25156.636929646647\n","--------------------\n","Column: Termd\n","  Range: 1\n","  Variance: 0.22329633855409128\n","  Standard Deviation: 0.4725424198461883\n","--------------------\n","Column: PositionID\n","  Range: 29\n","  Variance: 38.730940773778656\n","  Standard Deviation: 6.223418736818105\n","--------------------\n","Column: Zip\n","  Range: 97039\n","  Variance: 285893885.1924286\n","  Standard Deviation: 16908.396884164642\n","--------------------\n","Column: ManagerID\n","  Range: 38.0\n","  Variance: 65.25902126636504\n","  Standard Deviation: 8.078305593771818\n","--------------------\n","Column: EngagementSurvey\n","  Range: 3.88\n","  Variance: 0.6240012903225808\n","  Standard Deviation: 0.7899375230501339\n","--------------------\n","Column: EmpSatisfaction\n","  Range: 4\n","  Variance: 0.8267192199979254\n","  Standard Deviation: 0.9092410131521375\n","--------------------\n","Column: SpecialProjectsCount\n","  Range: 8\n","  Variance: 5.519780105798159\n","  Standard Deviation: 2.349421227834242\n","--------------------\n","Column: DaysLateLast30\n","  Range: 6\n","  Variance: 1.6757805206928689\n","  Standard Deviation: 1.2945194168852272\n","--------------------\n","Column: Absences\n","  Range: 19\n","  Variance: 34.252878332123174\n","  Standard Deviation: 5.8525958627025645\n","--------------------\n","\n","\n","Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Asian_American_Quality_of_Life.csv\n","Column: Survey ID\n","  Range: 1103499997\n","  Variance: 553195410616225.56\n","  Standard Deviation: 23520106.517960873\n","--------------------\n","Column: Age\n","  Range: 80.0\n","  Variance: 292.58945378399926\n","  Standard Deviation: 17.105246381856045\n","--------------------\n","Column: Education Completed\n","  Range: 17.0\n","  Variance: 5.939825745106607\n","  Standard Deviation: 2.437175772304207\n","--------------------\n","Column: Household Size\n","  Range: 7.0\n","  Variance: 2.1679732728676298\n","  Standard Deviation: 1.4724039095532278\n","--------------------\n","Column: Grandparent\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Other Relative\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Other \n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Self Employed Full Time\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Self Employed Part Time\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Disabled\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Unemployed\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Other Employement\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Achieving Ends Meet\n","  Range: 1.0\n","  Variance: 0.14263481137657338\n","  Standard Deviation: 0.3776702415819565\n","--------------------\n","Column: Duration of Residency\n","  Range: 77.75\n","  Variance: 162.38475799811084\n","  Standard Deviation: 12.743027819090361\n","--------------------\n","Column: Primary Language\n","  Range: 1.0\n","  Variance: 0.22200632499469306\n","  Standard Deviation: 0.471175471554593\n","--------------------\n","Column: Discrimination \n","  Range: 1.0\n","  Variance: 0.2112012785442446\n","  Standard Deviation: 0.45956640275834415\n","--------------------\n","Column: Hygiene Assistance\n","  Range: 1.0\n","  Variance: 0.026336393730207908\n","  Standard Deviation: 0.16228491528853786\n","--------------------\n","Column: Smoking\n","  Range: 1.0\n","  Variance: 0.05764349020432598\n","  Standard Deviation: 0.2400905874963156\n","--------------------\n","Column: Drinking\n","  Range: 1.0\n","  Variance: 0.032114484953418315\n","  Standard Deviation: 0.17920514767555734\n","--------------------\n","Column: Regular Exercise\n","  Range: 1.0\n","  Variance: 0.23619678711282296\n","  Standard Deviation: 0.48600080978618027\n","--------------------\n","Column: Healthy Diet\n","  Range: 1.0\n","  Variance: 0.15479985324317874\n","  Standard Deviation: 0.39344612495636394\n","--------------------\n","Column: Heart Disease\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Stroke\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Cancer\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Hepatitis\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Kidney Problem\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Asthma\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: COPD\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: users\n","  Range: 1.0\n","  Variance: 0.15761880479301782\n","  Standard Deviation: 0.3970123484137714\n","--------------------\n","Column: Other\n","  Range: 1.0\n","  Variance: 0.043213344881902246\n","  Standard Deviation: 0.20787819722592904\n","--------------------\n","Column: Quality of Life\n","  Range: 10.0\n","  Variance: 2.6716013903713733\n","  Standard Deviation: 1.6345034078800122\n","--------------------\n","Column: Psychiatrist\n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: Therapist/Counselor \n","  Range: 0.0\n","  Variance: 0.0\n","  Standard Deviation: 0.0\n","--------------------\n","Column: See Family\n","  Range: 5.0\n","  Variance: 1.7660808286071716\n","  Standard Deviation: 1.3289397385160742\n","--------------------\n","Column: Close Family\n","  Range: 5.0\n","  Variance: 1.6038740029133551\n","  Standard Deviation: 1.2664414723599962\n","--------------------\n","Column: Helpful Family\n","  Range: 5.0\n","  Variance: 1.5684397405569055\n","  Standard Deviation: 1.252373642551178\n","--------------------\n","Column: See Friends\n","  Range: 5.0\n","  Variance: 1.908956009166024\n","  Standard Deviation: 1.3816497418542892\n","--------------------\n","Column: Close Friends\n","  Range: 5.0\n","  Variance: 1.675487235603701\n","  Standard Deviation: 1.2944061324034666\n","--------------------\n","Column: Helpful Friends\n","  Range: 5.0\n","  Variance: 1.7498076387222572\n","  Standard Deviation: 1.3228029478052494\n","--------------------\n","Column: Residency \n","  Range: 54.75\n","  Variance: 94.61426165067175\n","  Standard Deviation: 9.726986257349793\n","--------------------\n","Column: Other Transportation\n","  Range: 1.0\n","  Variance: 0.010006715267229986\n","  Standard Deviation: 0.10003357070119004\n","--------------------\n","\n","\n","Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Provisional_COVID-19_Deaths.csv\n","Column: Year\n","  Range: 2.0\n","  Variance: 0.6572084960094967\n","  Standard Deviation: 0.8106839680229878\n","--------------------\n","Column: Month\n","  Range: 11.0\n","  Variance: 11.36828120576979\n","  Standard Deviation: 3.3716881833541175\n","--------------------\n","Column: COVID-19 Deaths\n","  Range: 1067539.0\n","  Variance: 40168399.89827592\n","  Standard Deviation: 6337.854518547733\n","--------------------\n","Column: Total Deaths\n","  Range: 9521311.0\n","  Variance: 2598066746.57542\n","  Standard Deviation: 50971.23450119116\n","--------------------\n","Column: Pneumonia Deaths\n","  Range: 987383.0\n","  Variance: 35864863.023176484\n","  Standard Deviation: 5988.727997093914\n","--------------------\n","Column: Pneumonia and COVID-19 Deaths\n","  Range: 540649.0\n","  Variance: 10400682.962002028\n","  Standard Deviation: 3225.0089863443836\n","--------------------\n","Column: Influenza Deaths\n","  Range: 12621.0\n","  Variance: 5756.193161047854\n","  Standard Deviation: 75.86957994511275\n","--------------------\n","Column: Pneumonia, Influenza, or COVID-19 Deaths\n","  Range: 1525057.0\n","  Variance: 84499765.54533052\n","  Standard Deviation: 9192.375402763451\n","--------------------\n","\n","\n"]}]},{"cell_type":"code","source":["# prompt: In the above code, discuss about the selected attribute(s) for the purpose and rationale behind it within 200- 250 words for each dataset in the above result\n","\n","The provided code calculates the range, variance, and standard deviation for numerical columns in multiple datasets.  Let's analyze the rationale behind these choices:\n","\n","**Range:**\n","\n","The *range* provides a simple measure of the spread of the data, representing the difference between the maximum and minimum values.  It's easy to understand and calculate but highly sensitive to outliers.  A large range indicates a wide dispersion of values, while a small range suggests that values are clustered closely together. In the context of these datasets, the range helps visualize the extent of variation within each numerical attribute. For instance, a large range in income would indicate a significant disparity in income levels within the dataset.\n","\n","**Variance:**\n","\n","The *variance* measures the average squared deviation of each data point from the mean. It quantifies the dispersion or spread of data points around the mean. A higher variance indicates a greater spread of data, meaning the data points are farther away from the mean on average. Variance is less susceptible to outliers compared to range but can be influenced by the units of measurement. For datasets like the Asian American Quality of Life or the Gender Pay Gap dataset, variance could reveal how dispersed the quality of life indicators or salaries are, respectively. A higher variance in salaries, for instance, would suggest greater inequality in income levels.\n","\n","\n","**Standard Deviation:**\n","\n","The *standard deviation* is the square root of the variance.  It expresses the data's spread in the same units as the original data, making it more interpretable than variance. Like variance, it reflects how much the data points deviate from the mean.  A larger standard deviation means the data is more spread out. For example, a large standard deviation in COVID-19 deaths could indicate significant fluctuations in mortality across different regions or time periods. Similarly, in the HR dataset, a higher standard deviation in salary might signify a greater disparity in compensation among employees.\n","\n","**Rationale for Selecting These Measures:**\n","\n","These three measures (range, variance, and standard deviation) provide a comprehensive understanding of the dispersion or spread of data around the central tendency.  While the *range* offers a simple measure of spread, *variance* and *standard deviation* are more robust to outlier effects and provide a more nuanced view of how data points are distributed. *Standard deviation*, expressed in the same units as the original data, is particularly helpful for interpretation.\n","\n","Together, range, variance, and standard deviation provide valuable information about the variability within each numerical attribute across datasets, allowing a more detailed analysis than central tendency measures alone.  They are essential for understanding the distribution and spread of the data. The code's check for numerical columns ensures that these calculations are applied appropriately and that errors due to non-numeric data are avoided.\n"],"metadata":{"id":"dUEOI2HGXm76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Selecting at least TWO attributes appropriately to construct confidence intervals with the above data sets and wit their paths\n","\n","import pandas as pd\n","import numpy as np\n","from scipy import stats\n","\n","def calculate_confidence_interval(data, confidence=0.95):\n","    \"\"\"Calculates the confidence interval for a given dataset.\"\"\"\n","    if len(data) < 2:  # need at least two data points\n","        return None\n","    a = 1.0 * np.array(data)\n","    n = len(a)\n","    m, se = np.mean(a), stats.sem(a)\n","    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n","    return m-h, m+h\n","\n","# Load the datasets (replace with your actual file paths)\n","file_paths = [\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Gender pay gap.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/HR_Dataset.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Asian_American_Quality_of_Life.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Provisional_COVID-19_Deaths.csv'\n","]\n","\n","for file_path in file_paths:\n","    try:\n","        df = pd.read_csv(file_path)\n","        print(f\"Analyzing: {file_path}\")\n","\n","        # Select numerical columns (excluding object types which are likely categorical)\n","        numerical_cols = df.select_dtypes(include=['number']).columns\n","\n","        for col in numerical_cols:\n","            if len(df[col].dropna()) >= 2 : # Check if there are at least two numerical values to avoid errors.\n","                # Calculate the confidence interval\n","                data = df[col].dropna()\n","                confidence_interval = calculate_confidence_interval(data)\n","\n","                if confidence_interval:\n","                    lower_bound, upper_bound = confidence_interval\n","                    print(f\"Column: {col}\")\n","                    print(f\"  95% Confidence Interval: ({lower_bound:.2f}, {upper_bound:.2f})\")\n","                    print(\"-\" * 20)\n","        print(\"\\n\")\n","    except FileNotFoundError:\n","        print(f\"Error: File not found at {file_path}\")\n","    except Exception as e:\n","        print(f\"An error occurred while processing {file_path}: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDzOCnUPShbk","executionInfo":{"status":"ok","timestamp":1744902705839,"user_tz":-60,"elapsed":602,"user":{"displayName":"Murali Dheekonda","userId":"09835857048925522524"}},"outputId":"91cfd2ce-6e28-4290-c5ab-200de5bebf84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Gender pay gap.csv\n","Column: age\n","  95% Confidence Interval: (40.51, 42.28)\n","--------------------\n","Column: perfEval\n","  95% Confidence Interval: (2.95, 3.13)\n","--------------------\n","Column: seniority\n","  95% Confidence Interval: (2.88, 3.06)\n","--------------------\n","Column: basePay\n","  95% Confidence Interval: (92900.34, 96044.96)\n","--------------------\n","Column: bonus\n","  95% Confidence Interval: (6342.78, 6591.54)\n","--------------------\n","\n","\n","Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/HR_Dataset.csv\n","Column: EmpID\n","  95% Confidence Interval: (10145.97, 10166.03)\n","--------------------\n","Column: MarriedID\n","  95% Confidence Interval: (0.34, 0.45)\n","--------------------\n","Column: MaritalStatusID\n","  95% Confidence Interval: (0.71, 0.92)\n","--------------------\n","Column: GenderID\n","  95% Confidence Interval: (0.38, 0.49)\n","--------------------\n","Column: EmpStatusID\n","  95% Confidence Interval: (2.19, 2.59)\n","--------------------\n","Column: DeptID\n","  95% Confidence Interval: (4.49, 4.73)\n","--------------------\n","Column: PerfScoreID\n","  95% Confidence Interval: (2.91, 3.04)\n","--------------------\n","Column: FromDiversityJobFairID\n","  95% Confidence Interval: (0.06, 0.13)\n","--------------------\n","Column: Salary\n","  95% Confidence Interval: (66213.83, 71827.54)\n","--------------------\n","Column: Termd\n","  95% Confidence Interval: (0.28, 0.39)\n","--------------------\n","Column: PositionID\n","  95% Confidence Interval: (16.15, 17.54)\n","--------------------\n","Column: Zip\n","  95% Confidence Interval: (4668.93, 8442.04)\n","--------------------\n","Column: ManagerID\n","  95% Confidence Interval: (13.66, 15.48)\n","--------------------\n","Column: EngagementSurvey\n","  95% Confidence Interval: (4.02, 4.20)\n","--------------------\n","Column: EmpSatisfaction\n","  95% Confidence Interval: (3.79, 3.99)\n","--------------------\n","Column: SpecialProjectsCount\n","  95% Confidence Interval: (0.96, 1.48)\n","--------------------\n","Column: DaysLateLast30\n","  95% Confidence Interval: (0.27, 0.56)\n","--------------------\n","Column: Absences\n","  95% Confidence Interval: (9.58, 10.89)\n","--------------------\n","\n","\n","Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Asian_American_Quality_of_Life.csv\n","Column: Survey ID\n","  95% Confidence Interval: (1202894.49, 3008745.56)\n","--------------------\n","Column: Age\n","  95% Confidence Interval: (42.19, 43.51)\n","--------------------\n","Column: Education Completed\n","  95% Confidence Interval: (14.98, 15.17)\n","--------------------\n","Column: Household Size\n","  95% Confidence Interval: (3.24, 3.35)\n","--------------------\n","Column: Grandparent\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Other Relative\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Other \n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Self Employed Full Time\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Self Employed Part Time\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Disabled\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Unemployed\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Other Employement\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Achieving Ends Meet\n","  95% Confidence Interval: (0.16, 0.19)\n","--------------------\n","Column: Duration of Residency\n","  95% Confidence Interval: (15.14, 16.13)\n","--------------------\n","Column: Primary Language\n","  95% Confidence Interval: (0.65, 0.69)\n","--------------------\n","Column: Discrimination \n","  95% Confidence Interval: (0.28, 0.32)\n","--------------------\n","Column: Hygiene Assistance\n","  95% Confidence Interval: (0.02, 0.03)\n","--------------------\n","Column: Smoking\n","  95% Confidence Interval: (0.05, 0.07)\n","--------------------\n","Column: Drinking\n","  95% Confidence Interval: (0.03, 0.04)\n","--------------------\n","Column: Regular Exercise\n","  95% Confidence Interval: (0.60, 0.64)\n","--------------------\n","Column: Healthy Diet\n","  95% Confidence Interval: (0.79, 0.82)\n","--------------------\n","Column: Heart Disease\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Stroke\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Cancer\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Hepatitis\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Kidney Problem\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Asthma\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: COPD\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: users\n","  95% Confidence Interval: (0.79, 0.82)\n","--------------------\n","Column: Other\n","  95% Confidence Interval: (0.04, 0.05)\n","--------------------\n","Column: Quality of Life\n","  95% Confidence Interval: (7.61, 7.73)\n","--------------------\n","Column: Psychiatrist\n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: Therapist/Counselor \n","  95% Confidence Interval: (0.00, 0.00)\n","--------------------\n","Column: See Family\n","  95% Confidence Interval: (3.03, 3.14)\n","--------------------\n","Column: Close Family\n","  95% Confidence Interval: (2.42, 2.52)\n","--------------------\n","Column: Helpful Family\n","  95% Confidence Interval: (2.76, 2.86)\n","--------------------\n","Column: See Friends\n","  95% Confidence Interval: (3.14, 3.25)\n","--------------------\n","Column: Close Friends\n","  95% Confidence Interval: (2.40, 2.50)\n","--------------------\n","Column: Helpful Friends\n","  95% Confidence Interval: (2.64, 2.74)\n","--------------------\n","Column: Residency \n","  95% Confidence Interval: (8.89, 9.66)\n","--------------------\n","Column: Other Transportation\n","  95% Confidence Interval: (0.01, 0.01)\n","--------------------\n","\n","\n","Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Provisional_COVID-19_Deaths.csv\n","Column: Year\n","  95% Confidence Interval: (2020.97, 2020.98)\n","--------------------\n","Column: Month\n","  95% Confidence Interval: (6.32, 6.36)\n","--------------------\n","Column: COVID-19 Deaths\n","  95% Confidence Interval: (322.71, 411.40)\n","--------------------\n","Column: Total Deaths\n","  95% Confidence Interval: (2501.23, 3160.31)\n","--------------------\n","Column: Pneumonia Deaths\n","  95% Confidence Interval: (315.50, 401.67)\n","--------------------\n","Column: Pneumonia and COVID-19 Deaths\n","  95% Confidence Interval: (161.76, 206.57)\n","--------------------\n","Column: Influenza Deaths\n","  95% Confidence Interval: (3.03, 4.03)\n","--------------------\n","Column: Pneumonia, Influenza, or COVID-19 Deaths\n","  95% Confidence Interval: (482.21, 613.85)\n","--------------------\n","\n","\n"]}]},{"cell_type":"code","source":["\n","\n","The provided code calculates confidence intervals for numerical columns in multiple datasets.\n","The choice of attributes is driven by the need to estimate the range within which the true population parameter (e.g., mean) is likely to fall.\n","The rationale behind using confidence intervals is to provide a measure of uncertainty around sample statistics.\n","\n","\n","Rationale for Confidence Intervals:\n","\n","1.  Uncertainty Quantification:  A sample statistic (e.g., the mean of a sample) is an estimate of the true population parameter.\n","     A confidence interval acknowledges that the sample statistic is not perfectly accurate and provides a range within which the true population parameter is likely to lie.\n","\n","\n"," 2.  Hypothesis Testing:  Confidence intervals are closely related to hypothesis testing.  If a hypothesized value falls outside the confidence interval,\n","     it suggests that the hypothesis is unlikely to be true.\n","\n","\n"," 3.  Data Variability:  The width of the confidence interval reflects the variability in the data. A wider interval indicates more uncertainty,\n","     while a narrower interval suggests greater precision in the estimate.\n","\n","\n"," 4.  Sample Size:  The sample size influences the width of the confidence interval.  Larger sample sizes typically result in narrower intervals because they provide more precise estimates of the population parameter.\n","\n"," 5.  Population Distribution:  The assumption about the underlying population distribution (usually normality) impacts the calculation of the confidence interval.  For large sample sizes, the Central Limit Theorem ensures that the sample mean will be approximately normally distributed, regardless of the underlying population distribution.\n","\n","\n"," Attribute Selection and Interpretation:\n","\n"," In this code, confidence intervals are calculated for all numerical columns in each dataset.  The specific interpretation of these intervals depends on the context of each column within its dataset:\n","\n"," 1. Gender Pay Gap Dataset:  Confidence intervals around salary (`basePay` or similar attributes) for each gender would help determine if there's a statistically significant difference in mean salaries between genders.  A non-overlapping confidence interval would suggest a significant difference.\n","\n","\n"," 2.  HR Dataset:  Confidence intervals around `Salary`, `Absences`, or other numerical features could be used to understand the uncertainty associated with the average values.  This helps in drawing conclusions about the employees in this organization.\n","\n"," 3.  Asian American Quality of Life Dataset: Confidence intervals would provide a sense of precision for the calculated means of different quality-of-life indicators, such as income or education levels.  These intervals help us understand whether the averages observed in the sample are good representations of the actual values in the population.\n","\n"," 4.  Provisional COVID-19 Deaths Dataset: Confidence intervals around the number of deaths would estimate the range within which the true population mean lies, taking into account the sampling variability.\n","\n"," In summary, by calculating confidence intervals for various numerical attributes, we not only obtain point estimates but also quantify our confidence in these estimates, enabling more robust conclusions about the datasets.\n","\n","\n"],"metadata":{"id":"bvyQKhx5YLg6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: Selecting at least TWO attributes appropriately to perform hypothesis testing\n","# perform with above data sets with their paths\n","\n","import pandas as pd\n","from scipy import stats\n","\n","# Load the datasets\n","file_paths = [\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Gender pay gap.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/HR_Dataset.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Asian_American_Quality_of_Life.csv',\n","    '/content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Provisional_COVID-19_Deaths.csv'\n","]\n","\n","for file_path in file_paths:\n","    try:\n","        df = pd.read_csv(file_path)\n","        print(f\"Analyzing: {file_path}\")\n","\n","        # Select two numerical columns for hypothesis testing (replace with your choices)\n","        #  Ensure these columns are appropriate for the chosen test (e.g., normally distributed)\n","        #  and that there are enough samples\n","\n","        numerical_cols = df.select_dtypes(include=['number']).columns\n","        if len(numerical_cols) >= 2:  # Check if at least two numerical columns exist\n","            col1 = numerical_cols[0]\n","            col2 = numerical_cols[1]\n","\n","            # Perform an independent samples t-test (assuming two groups)\n","            # Adapt the test based on your specific hypothesis and data\n","            if len(df[col1].dropna()) >= 2 and len(df[col2].dropna()) >= 2: # Check for sufficient data points.\n","                t_statistic, p_value = stats.ttest_ind(df[col1].dropna(), df[col2].dropna())\n","\n","                print(f\"T-test for {col1} vs. {col2}\")\n","                print(f\"  T-statistic: {t_statistic:.3f}\")\n","                print(f\"  P-value: {p_value:.3f}\")\n","\n","                alpha = 0.05  # Significance level\n","                if p_value < alpha:\n","                    print(\"  Reject the null hypothesis: There is a statistically significant difference between the means.\")\n","                else:\n","                    print(\"  Fail to reject the null hypothesis: There is no statistically significant difference between the means.\")\n","            else:\n","                print(f\"Insufficient data points in {col1} or {col2} to perform t-test\")\n","        else:\n","            print(f\"Not enough numerical columns in {file_path} to perform hypothesis testing.\")\n","        print(\"-\" * 20)\n","        print(\"\\n\")\n","    except FileNotFoundError:\n","        print(f\"Error: File not found at {file_path}\")\n","    except Exception as e:\n","        print(f\"An error occurred while processing {file_path}: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDlqY-dcT9uR","executionInfo":{"status":"ok","timestamp":1744903221817,"user_tz":-60,"elapsed":1446,"user":{"displayName":"Murali Dheekonda","userId":"09835857048925522524"}},"outputId":"64dd0c53-02e8-4f1f-a943-8c14954e5698"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Gender pay gap.csv\n","T-test for age vs. perfEval\n","  T-statistic: 84.432\n","  P-value: 0.000\n","  Reject the null hypothesis: There is a statistically significant difference between the means.\n","--------------------\n","\n","\n","Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/HR_Dataset.csv\n","T-test for EmpID vs. MarriedID\n","  T-statistic: 1991.648\n","  P-value: 0.000\n","  Reject the null hypothesis: There is a statistically significant difference between the means.\n","--------------------\n","\n","\n","Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Asian_American_Quality_of_Life.csv\n","T-test for Survey ID vs. Age\n","  T-statistic: 4.565\n","  P-value: 0.000\n","  Reject the null hypothesis: There is a statistically significant difference between the means.\n","--------------------\n","\n","\n","Analyzing: /content/drive/MyDrive/Colab Notebooks/Mathematics for Data Science/Provisional_COVID-19_Deaths.csv\n","T-test for Year vs. Month\n","  T-statistic: 187500.793\n","  P-value: 0.000\n","  Reject the null hypothesis: There is a statistically significant difference between the means.\n","--------------------\n","\n","\n"]}]},{"cell_type":"code","source":["\n","The provided code analyzes multiple datasets using a variety of statistical methods.  The selection of attributes and statistical tests depends heavily on the nature of the data within each dataset and the research questions being addressed. The rationale behind the attribute and statistical test selections are discussed below.\n","\n","**Descriptive Statistics (Mean, Median, Mode, Range, Variance, Standard Deviation):**\n","\n","These measures provide a basic understanding of the data's central tendency and dispersion. The choice of attributes is generally straightforward, as they apply to numerical columns within each dataset. The rationale behind each statistic:\n","\n","* **Mean:** Represents the average value, giving a sense of the typical value but is sensitive to outliers.\n","* **Median:** The middle value when the data is sorted.  More robust than the mean to outliers.\n","* **Mode:** Identifies the most frequent value; particularly helpful for categorical or discrete data.\n","* **Range:**  A simple measure of spread; difference between maximum and minimum values, however, highly sensitive to outliers.\n","* **Variance & Standard Deviation:**  Quantify the spread of data around the mean. Standard deviation is more interpretable because it's in the original data units.\n","\n","**Confidence Intervals:**\n","\n","Confidence intervals provide a range of values within which the true population parameter (like the mean) likely lies, given the sample data.  The width of the interval reflects uncertainty.  Attributes for confidence intervals are again numerical variables.  A key rationale is to understand the uncertainty associated with sample statistics.\n","\n","**Hypothesis Testing (T-test):**\n","\n","Hypothesis testing determines whether there's a statistically significant difference between the means of two groups.  In this case, the code performs an independent samples t-test.  The *crucial* requirement for a t-test is that the data should approximately follow a normal distribution.  Attribute selection hinges on identifying two numerical columns that are suitable for comparison (e.g., income differences between two demographic groups). The rationale behind the t-test is to determine whether any observed difference between the groups is likely due to chance or a real effect.  Failing to meet the normality assumption could lead to inaccurate results.  The p-value helps determine if the difference is statistically significant.\n","\n","**Overall Rationale:**\n","\n","1.  **Exploratory Data Analysis:** Descriptive statistics (mean, median, mode, range, variance, standard deviation) and confidence intervals initially help to understand the data’s central tendency, dispersion, and uncertainty.\n","\n","2.  **Comparative Analysis:** Hypothesis testing allows comparison between two groups and assesses whether any differences are statistically significant.\n","\n","3.  **Dataset Specificity:** Appropriate attribute selection is paramount.  For instance, using the t-test on non-normally distributed data or on data that violate the other assumptions of the test would lead to incorrect conclusions.  Appropriate checks and alternative tests are needed if these assumptions are not met.  Careful data exploration should precede hypothesis tests to understand the data distribution and confirm or reject these assumptions.\n"],"metadata":{"id":"x_7HVTMvYPPZ"},"execution_count":null,"outputs":[]}]}